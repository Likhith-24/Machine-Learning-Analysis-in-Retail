{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aed43dd-77a8-4d0a-a8bb-7da17f425e77",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126c837-f827-4b29-9115-823ef8a990e9",
   "metadata": {},
   "source": [
    "## **Advanced Machine Learning Analysis in Retail**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b4317-fa09-48bb-9bb9-fd2dc6d8fa5f",
   "metadata": {},
   "source": [
    "Estimated time needed: **2** hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16451f46-7e0c-4a60-894b-ea983fffb22e",
   "metadata": {},
   "source": [
    "## Abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc4467-b49c-4aff-baa5-098fbfd72428",
   "metadata": {},
   "source": [
    "This lab is dedicated to learning the Advanced Machine Learning methods for analysis of Retail based on historical sales data for 45 stores located in different regions - each store contains a number of departments. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01153fa6-9e90-42cb-9b02-0d5d89c1a489",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7430c7ac-194b-4c70-bb39-5e63d4c7c501",
   "metadata": {},
   "source": [
    "Making decisions based on limited history is one of the challenges of modeling retail data. Holidays and major events come once a year, and so does the chance to see how strategic decisions impacted the bottom line. In addition, markdowns are known to affect sales – the challenge of this lab is to predict which departments will be affected and to what extent.\n",
    "Therefore, the main problem to be solved in this lab is the use of advanced methods of machine learning to:\n",
    "\n",
    "1. predict the department-wide sales for each store;\n",
    "2. model the effects of markdowns on holiday weeks;\n",
    "3. provide recommended actions based on the insights drawn, with prioritization placed on largest business impact.\n",
    "\n",
    "This lab shows the use of a set of machine learning methods to solve such problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934b792e-b129-4e9d-8781-8fd083e7c014",
   "metadata": {},
   "source": [
    "## Materials and methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ad47b-4a50-4739-b1bf-e4cd1dbd4493",
   "metadata": {},
   "source": [
    "In this lab, we will learn how to analyze and forecast store sales.\n",
    "We will study how to use autocorrelation analysis to find time lag delays and how to transform a DataSet to take it into account.\n",
    "We will study how to use a set of different ML models to predict time series through the example of the department week sales.\n",
    "\n",
    "On the basis of a neural network, we will analyze how markdowns in the store influence the sales both during the holiday and regular weeks. After that, a sales strategy for a specific department will be proposed.\n",
    "\n",
    "This lab consists of the following steps:\n",
    "* Import Libraries/Define Auxiliary Functions\n",
    "* Download and pre-preparation data \n",
    "* Predict the department-wide sales \n",
    "    - Previous Data Analysis\n",
    "    - DataSet creation\n",
    "    - Data normalization\n",
    "    - Linear Regression\n",
    "    - Back Propagation Neural Network\n",
    "    - Long Short-Term Memory - LSTM\n",
    "* Model the effects of markdowns on holiday weeks\n",
    "    - Preliminary analysis\n",
    "    - Linear Regression\n",
    "    - Back Propagation Neural Network\n",
    "    - Sensitivity analysis\n",
    "* Recommendation for department\n",
    "* Final Task\n",
    "    - SubTask 1. Sensitivity function\n",
    "    - SubTask 2. Sensitivity of Department\n",
    "    - SubTask 3. Sensitivity of 10 departments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e369c-54c6-40e2-9cd8-dd47ec969aac",
   "metadata": {},
   "source": [
    "The statistical data was obtained from the https://www.kaggle.com/manjeetsingh/retaildataset. This DataSet released under CC BY-IGO license that allow copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f2626d-c803-4339-a27f-93b39e140146",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* [Python](https://www.python.org) - advanced level\n",
    "* [Pandas](https://pandas.pydata.org) - basic level \n",
    "* [SeaBorn](https://seaborn.pydata.org) - basic level\n",
    "* [Scikit-learn](https://scikit-learn.org/stable/) - intermediate level\n",
    "* [keras](https://keras.io) - intermediate level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c0005a-a89e-432a-a128-77d745c0f5cc",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a184efbf-cf86-4a85-8bcb-b20d9b296981",
   "metadata": {},
   "source": [
    "After completing this lab, you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc139de-6d78-42fc-890a-cf8d5ee926c6",
   "metadata": {},
   "source": [
    "* Download a DataSet from *.csv files\n",
    "* Merge DataSets\n",
    "* Make autocorrelation analysis\n",
    "* Transform a DataSet considering Lag shift\n",
    "* Apply basic and advanced methods of machine learning such a: Linear regression, Back-Propagation and Recurrent Neural Networks\n",
    "* Calculate the accuracy of models\n",
    "* Make forecasting \n",
    "* Calculate and analyze the sensitivity of models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4542ff3-ac3b-4713-a7fa-c84c02dae536",
   "metadata": {},
   "source": [
    "## Import Libraries/Define Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a072215-afe9-46cd-9bde-976163c71758",
   "metadata": {},
   "source": [
    "**Running outside Skills Network Labs.** This notebook was tested within Skills Network Labs. Running in another environment should work as well, but is not guaranteed and may require different setup routine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58633d0e-0156-408a-ad01-5eb7056d0cd8",
   "metadata": {},
   "source": [
    "Libraries such as Pandas, MatplotLib, SeaBorn, Scikit-Learn, Keras and Tabulate should be installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ca8b3-52fa-4069-85d4-c1f26b433801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d222b2-2c2d-404c-8f5a-3d35eef8bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c80d75-c785-428b-8ac8-06a950400311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03393dec-3c69-470b-944f-4eeba4d143ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f1c264-3fa4-496a-9303-c0f4e9210046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b467d91c-6ef8-42ea-8e4d-37878c4e8b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd53a279-5716-42dd-a831-515d715e73c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b48a74-e714-4fa3-bc53-5d0085804c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c anaconda statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea959aa-98e3-4759-a7b2-e48883e4bc0b",
   "metadata": {},
   "source": [
    "## Download data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c967d03-90a2-4b0c-b43a-dfb26087729a",
   "metadata": {},
   "source": [
    "Some libraries should be imported before you can begin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39baf02-6111-46d1-998f-8aba51fa6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.graphics.tsaplots import acf, pacf, plot_acf, plot_pacf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edc54f6-af41-462c-9399-1b5ee4fbeefd",
   "metadata": {},
   "source": [
    "Let's download retail data that relate to the store, department, and regional activity for the given dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8002aa9-e017-401d-8a27-47b9259b9505",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0BOFEN/Features%20data%20set.csv', delimiter=',')\n",
    "df1.dataframeName = 'Features data set.csv'\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed024c1c-dc8f-4764-a12d-386ef68171bb",
   "metadata": {},
   "source": [
    "Let's study this DataSet. As you can see, the DataSet consists of 8 190 rows and 12 columns.\n",
    "\n",
    "- Store - the store number\n",
    "- Date - the week\n",
    "- Temperature - average temperature in the region\n",
    "- Fuel_Price - cost of fuel in the region\n",
    "- MarkDown1-5 - anonymized data related to promotional markdowns. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA\n",
    "- CPI - the consumer price index\n",
    "- Unemployment - the unemployment rate\n",
    "- IsHoliday - whether the week is a special holiday week\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3c3ecc-f521-4e14-8c51-0dd4741d50f2",
   "metadata": {},
   "source": [
    "Next, we should download historical sales data which covers the period from 2010-02-05 to 2012-11-01.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65568198-fcd4-40cd-83b6-133abca1cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0BOFEN/sales%20data-set.csv', delimiter=',')\n",
    "df2.dataframeName = 'Sales data set.csv'\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca71623c-0c08-48de-8691-e1521878adde",
   "metadata": {},
   "source": [
    "As you can see, this DataSet consists of 421 579 rows and 5 columns.\n",
    "\n",
    "Within this DataSet, you will find the following information:\n",
    "\n",
    "- Store - the store number\n",
    "- Dept - the department number\n",
    "- Date - the week\n",
    "- Weekly_Sales -  sales for the given department in the given store\n",
    "- IsHoliday - whether the week is a special holiday week\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d61799c-3bf8-40ff-9bfd-eade227a64f3",
   "metadata": {},
   "source": [
    "The last DataSet contains anonymized information about 45 stores, indicating the type and size of a store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33629342-f0a5-42c3-9550-0bbd2d2dd6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0BOFEN/stores%20data-set.csv', delimiter=',')\n",
    "df3.dataframeName = 'Stores data set.csv'\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee871d2-9cd8-4e8a-9e6d-07dd04f78a6b",
   "metadata": {},
   "source": [
    "## Data pre-preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183abdae-e15b-4807-a8bb-2cb931779a21",
   "metadata": {},
   "source": [
    "First of all, we need to merge these three DataSets into one using **[pandas.DataFrame.merge()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96cf20-71d3-4c03-84c4-81522851d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.merge(df3, on = 'Store')\n",
    "df = df2.merge(df, on = ['Store','Date', 'IsHoliday'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d95539-2bdf-4a88-85b3-aa24594f81a9",
   "metadata": {},
   "source": [
    "Let's study this DataSet. As you can see, it consists of 421 570 rows × 16 columns. The DataSet contains information of different types. We should make sure that Python recognized the data types correctly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b31a625-b126-499b-80c6-b887914e065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862c2ee8-0a45-4798-a08c-5053c65afadd",
   "metadata": {},
   "source": [
    "First of all, let's delete rows that contain empty values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f0aa3-52b5-479c-a468-2951735b9ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb2632a-a62e-4b62-ad76-7a6c2ae38fed",
   "metadata": {},
   "source": [
    "As you can see, we should transform the Date columns into the DateTime format. Also the type of Store should be categorical:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094646e-dc3b-418a-9f9e-a2b9ea9f7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Type'] = df['Type'].astype('category')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d60584-816f-4471-b1c1-1571e0c78f3d",
   "metadata": {},
   "source": [
    "**Since stores and their departments belong to different categories, have different sizes, different quantities and assortments of goods and are located in different parts of the city, it will be a mistake to fit the neural network on all records. Departments located in different parts of the city will have different sales with the same input data. In other words, the information for each department has its own variance. Therefore, for the analysis, it is necessary to identify departments and make an analysis for each of them individually.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782d7bb-1550-46e7-aa21-0eb5c08762e5",
   "metadata": {},
   "source": [
    "Let's group the Rows by Store, Department and Date. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea054b-cf88-4458-a0ac-96dbeff03c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Store', 'Dept','Date']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0398dd-1fbb-46dc-8c05-7e032c21aead",
   "metadata": {},
   "source": [
    "Let's calculate the number of rows for each department:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124c842-15c7-46cc-b029-8c5cbac853b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Store', 'Dept']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcce154-97b5-446b-adbf-e8a9741dbd97",
   "metadata": {},
   "source": [
    "As you can see, most of the departments have 143 rows. Let's analyze one of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4904a3-d7e8-469b-ace8-e70a3853fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "St = 24\n",
    "Dt = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff9ace-48ff-4048-86d0-a35c846a8c72",
   "metadata": {},
   "source": [
    "Let's create a DataSet for a Store: St and for a Department: Dt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ac5a9-98ca-4ca3-83de-641a4fc770aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = df[(df['Store']==St) & (df['Dept']==Dt)]\n",
    "df_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c39ce3e-031e-4f3d-9f6b-0ef140b170b3",
   "metadata": {},
   "source": [
    "## Predict the department-wide sales \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea1fe9-162e-426c-997f-3c3cc47d9dd2",
   "metadata": {},
   "source": [
    "### Previous Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3ee013-7f9e-4ab9-aa1e-7104953230e9",
   "metadata": {},
   "source": [
    "Let's take the field 'Weekly_Sales' for forecasting. First of all, we should visualize this data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a377c29c-e248-4b27-8e1d-17a1d91ad8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure\n",
    "_ = plt.subplots(figsize = (20,10))\n",
    "_ = plt.xticks(rotation = 60)\n",
    "_ = sns.lineplot(data = df_d, x = 'Date',y = 'Weekly_Sales', )\n",
    "_ = plt.title('LinePlot showing the change in Weekly Sales', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d3dfc-65e2-4366-8704-51d3a12f5534",
   "metadata": {},
   "source": [
    "Let's visualize how sales change during the holidays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f929308e-271e-4983-93a6-a4b00cc245f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure\n",
    "_ = plt.subplots(figsize = (20,10))\n",
    "_ = plt.xticks(rotation = 60)\n",
    "_ = sns.lineplot(data = df_d, x = 'Date',y = 'Weekly_Sales', hue = 'IsHoliday',style = 'IsHoliday', markers = True, ci = 68)\n",
    "_ = plt.title('LinePlot showing the change in Weekly Sales', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d96da9-4fea-4aab-bea2-35707154352d",
   "metadata": {},
   "source": [
    "As you can see from the plot, there is no increase in sales on holidays. \n",
    "\n",
    "For a sales forecast, let's create a separate time series that contains only weekly sales data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc987e-526a-4d32-adde-ae5d291a2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = df_d[['Date', 'Weekly_Sales']]\n",
    "ts = ts.set_index('Date')\n",
    "ts = ts['Weekly_Sales']\n",
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef57774-7014-4f88-a269-efcee2c61789",
   "metadata": {},
   "source": [
    "If we would like to make a forecast of time series, we can make only an assumption that the data for today depend on the values of previous weeks. In order to check for dependencies, it is necessary to perform a correlation analysis between them. This requires:\n",
    "1. duplicating the time series of data and moving it vertically down for a certain number of days (lag)\n",
    "2. deleting the missing data at the beginning and at the end (they are formed by vertical shift (**[pandas.DataFrame.shift()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html)**)\n",
    "3. calculating the correlation coefficient between the obtained series.\n",
    "\n",
    "Since this operation should be performed for different values of the lag, it is convenient to create a separate function or use **[statsmodels.graphics.tsaplots.plot_acf()](https://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_acf.html)**. \n",
    "\n",
    "Or better we can use [Partial autocorrelation function](https://en.wikipedia.org/wiki/Partial_autocorrelation_function): **[statsmodels.graphics.tsaplots.plot_pacf()](https://www.statsmodels.org/stable/generated/statsmodels.graphics.tsaplots.plot_pacf.html)**.\n",
    "\n",
    "This analysis will allow us to determine the lag delay. That is, how many weeks ago sales affected today's sales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4352e53-17db-48fe-9dcf-490b7185c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Series(acf(ts,nlags=10), name = \"Correlation Coeff\"))\n",
    "print(pd.Series(pacf(ts,nlags=10), name = \"Partial Correlation Coeff\"))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(20,5))\n",
    "_ = plot_acf(ts, lags=30, ax=axes[0])\n",
    "_ = plot_pacf(ts, lags=30, ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677e79ab-46e7-45a7-9a82-67d88dc33f7f",
   "metadata": {},
   "source": [
    "As can be seen from the charts, we have to use sales for the previous 4 weeks as input parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536fb96b-6d80-452f-84b1-3f9412905bdd",
   "metadata": {},
   "source": [
    "### DataSet creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150792fc-bf2c-4cd6-ac13-bd415c6b354d",
   "metadata": {},
   "source": [
    "Any forecast model can be shown as a black-box of input - target. The target should be the data of the original time series, and the input values are given for the previous weeks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33abf021-935e-4c5b-99e9-8632a0e20ebc",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0BOFEN/TS.png\" width=\"1000\" alt=\"cognitiveclass.ai logo\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9818e38c-d1ec-4637-a2ce-04a26c423ef3",
   "metadata": {},
   "source": [
    "To automate this process, let's create a general function for time series transformation into a dataset structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c36e4a-c8fa-4663-a4c9-abca7905192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(in_data, tar_data, n_in=1, dropnan=True, target_dep=False):\n",
    "    \"\"\"\n",
    "    Transformation into a training sample taking into account the lag\n",
    "     : param in_data: Input fields\n",
    "     : param tar_data: Output field (single)\n",
    "     : param n_in: Lag shift\n",
    "     : param dropnan: Do destroy empty lines\n",
    "     : param target_dep: Whether to take into account the lag of the input field If taken into account, the input will start with lag 1\n",
    "     : return: Training sample. The last field is the source\n",
    "    \"\"\"\n",
    "\n",
    "    n_vars = in_data.shape[1]\n",
    "    cols, names = list(), list()\n",
    "\n",
    "    if target_dep:\n",
    "        i_start = 1\n",
    "    else:\n",
    "        i_start = 0\n",
    "    for i in range(i_start, n_in + 1):\n",
    "        cols.append(in_data.shift(i))\n",
    "        names += [('%s(t-%d)' % (in_data.columns[j], i)) for j in range(n_vars)]\n",
    "\n",
    "    if target_dep:\n",
    "        for i in range(n_in, -1, -1):\n",
    "            cols.append(tar_data.shift(i))\n",
    "            names += [('%s(t-%d)' % (tar_data.name, i))]\n",
    "    else:\n",
    "        # put it all together\n",
    "        cols.append(tar_data)\n",
    "        names.append(tar_data.name)\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83240ce4-d14c-46c4-b604-a4929c008fcc",
   "metadata": {},
   "source": [
    "As mentioned above, the input and output fields are the same when predicting time series, they are only shifted by the lag.\n",
    "Let's create a dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72fecc5-7745-46f8-8d33-8046eb679ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = series_to_supervised(pd.DataFrame(ts), ts, 4)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc2f81-b2fd-42df-9739-269812251e5a",
   "metadata": {},
   "source": [
    "As you can see, the first and last columns contain the same target data. \n",
    "Now we should create input (**X**) and output (**Y**) Datasets for forecasting models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab126f5-412b-45eb-a484-bc984aba29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = dataset.columns\n",
    "X, Y = dataset[col[1:-1]], dataset[col[-1]]\n",
    "print(\"Input: \", X.columns)\n",
    "print(\"Target:\", Y.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b8f11-2588-48b3-9b24-fcd02c8da02b",
   "metadata": {},
   "source": [
    "### Data normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b13388-b19d-455d-b18d-324586aeaa96",
   "metadata": {},
   "source": [
    "After that, we should normalize all the data. In order to do this, the [**sklearn.preprocessing.MinMaxScaler**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) module should be used. \n",
    "It allows easy normalize [**fit_transform()**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.fit_transform) and convert back all data: [**fit_transform()**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.inverse_transform) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece74ed-df0b-4677-b08b-398372b02fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scaled_x = scaler_x.fit_transform(X)\n",
    "scaled_y = scaler_y.fit_transform(Y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9da626-9d34-4afd-a060-dc77f1499d51",
   "metadata": {},
   "source": [
    "After that we will form a training and a test DataSet using [**sklearn.model_selection.train_test_split()**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) in the ratio of 70/30. Without shuffling. It means that test samples are located at the end of **X** and **Y** DataSets.\n",
    "\n",
    "As a result we will have: \n",
    "\n",
    "Input normalized DataSets: **X_train, X_test**\n",
    "\n",
    "Target normalized DataSets: **y_train, y_test**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289968ee-c9e9-47f3-8c12-70e6c35869de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_x, scaled_y, test_size=0.3, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96966685-2c92-4364-900e-2c6ab7920d72",
   "metadata": {},
   "source": [
    "All the data is normalized. However, for comparing results we should have real scale data of the training and test DataSets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dea5a0-5670-4ed3-b0a3-a3f9244652ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train = scaler_y.inverse_transform(y_train).flatten()\n",
    "res_test = scaler_y.inverse_transform(y_test).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553a0c32-d80c-4b05-8549-70b95c5b9c94",
   "metadata": {},
   "source": [
    "Target real scale DataSets: **res_train, res_test**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14677e-f6c3-44a6-b72e-a565ec883c68",
   "metadata": {},
   "source": [
    "### Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3368e4a4-4ba4-4b5c-bb56-c249e28897df",
   "metadata": {},
   "source": [
    "First of all, we should create a model. We will test three types of models. Linear regression, Multilayer Neural Network with Backpropagation and Long Short-Term Memory Neural Network.\n",
    "Let's create a [**LinearRegression()**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb56b1b-7d4f-4fee-9366-0e076842dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9c4973-91f6-437a-a87c-defc36bfa4cd",
   "metadata": {},
   "source": [
    "After that, the model should be fitted on the training DataSet. In order to do this, we will use the function fit().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f4bd62-ab54-41f1-9fa9-d1631abfc3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018e1ed2-0517-4779-a921-dfe2f25d3368",
   "metadata": {},
   "source": [
    "Then we can test it on the test DataSet and use it for prognostication. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed0da55-1f87-48ad-925b-6065405ebb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_ln = regressor.predict(x_test)\n",
    "y_pred_test_ln = scaler_y.inverse_transform(y_pred_test_ln).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0802d5-4830-44cf-b4d4-756cf1459908",
   "metadata": {},
   "source": [
    "Let's analyze the accuracy of the results using **[sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a98f7-6ca5-4bde-a641-4cc99af60ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation train\", regressor.score(x_train, y_train))\n",
    "print(\"Correlation test\", regressor.score(x_test, y_test))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_test_ln))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_test_ln))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_ln)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba1de7-47be-4dbb-b1e4-9a583485ae31",
   "metadata": {},
   "source": [
    "As you can see, the result correlation on the test DataSet is very bad. Therefore, we should use another nonlinear model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f16db-58bb-4392-9a5e-f73c189f8e72",
   "metadata": {},
   "source": [
    "### Back Propagation Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc854728-0d2a-4fda-9aa6-8b93819be2db",
   "metadata": {},
   "source": [
    "The modern approach to the establishment of complex functional dependencies is the use of neural networks. A classical neural network is a [**multilayer neural network with back propagation**](https://en.wikipedia.org/wiki/Backpropagation).\n",
    "\n",
    "We will use [**keras**](https://keras.io) framework to build this model.\n",
    "First of all, we should create a Neural Network model as a separate function.\n",
    "\n",
    "A neural network is a sequence of layers. The function [**Sequential()**](https://keras.io/guides/sequential_model/) is used to create a network.\n",
    "\n",
    "Let's create a network that consists of 2 hidden layers. Each of which consists of 100 neurons. [**keras.layers.Dense()**](https://keras.io/api/layers/core_layers/dense/).\n",
    "\n",
    "To avoid retraining problems, we will use additional layers [**keras.layers.Dropout()**](https://keras.io/api/layers/regularization_layers/dropout/).\n",
    "\n",
    "The output layer will consist of one neuron, since we have only one value at the output.\n",
    "\n",
    "Model should be compiled for fitting and predicting: [**keras.Model.compile()**](https://keras.io/api/models/model_training_apis/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51256f0-48a2-4ef6-8958-ad0fc268af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BP_model(X):\n",
    "    \"\"\"\n",
    "    Multilayer neural network with back propagation .\n",
    "    :param X: Input DataSet\n",
    "    :return: keras NN model\n",
    "    \"\"\"\n",
    "    # create model\n",
    "    model = Sequential() \n",
    "    model.add(Dense(100, input_dim=X.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc68a6-5eb0-4e67-9b15-3ee80b7b3956",
   "metadata": {},
   "source": [
    "Once the model function is built, it is necessary to create a neural network directly and specify the learning parameters: [**keras.wrappers.scikit_learn.KerasRegressor()**](https://keras.io/zh/scikit-learn-api/). Also we should specify the number of fitting [**epoch and batch size**](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432186b-1b2b-471e-9f0c-baaab1b63cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size=int(y_train.shape[0]*.1)\n",
    "estimator = KerasRegressor(build_fn=BP_model, X=x_train, epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc3c27-cf43-46da-99b4-68543806222e",
   "metadata": {},
   "source": [
    "Now, let’s train our model for **1000** epochs.\n",
    "It should be noted, that fitting process is very slow. To avoid overfitting and decrease the time of fitting, we will use **[EarlyStopping()](https://keras.io/api/callbacks/early_stopping/)** function, that will control the value of the loss function. This function will stop the fitting process when the loss function stop decreasing during 10 iteration. After that, there will be a rollback of all weight parameters to their state that was 10 iteration before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8107bd-433a-4ab1-8023-d8a223c915fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='auto', patience=10, verbose=1,  restore_best_weights=True)\n",
    "history=estimator.fit(x_train,y_train, validation_data=(x_test,y_test), callbacks=[es]) # Fitting model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898b30b6-dbe5-4462-b529-874d7bd424f8",
   "metadata": {},
   "source": [
    "Let's show [**loss and validation loss dynamics**](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11507fc-ea8f-4b6d-bc8b-c7593c01abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c26104-d17a-4d85-b6ca-966725f720a2",
   "metadata": {},
   "source": [
    "As you can see, Neural Network is fitting well and no overfitting is observed.\n",
    "Let's calculate prediction of the training (**res_train_ANN**) and test (**res_test_ANN**) sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db2cee-58ee-4b7c-a3ca-f2951a45f19f",
   "metadata": {},
   "source": [
    "Let's calculate the forecast and make inverse normalization to real scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ed53d-aabe-4504-90ad-e09bffcae4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tr=estimator.predict(x_train)\n",
    "res_ts=estimator.predict(x_test)\n",
    "res_train_ANN=scaler_y.inverse_transform(res_tr.reshape(-1, 1)).flatten()\n",
    "res_test_ANN=scaler_y.inverse_transform(res_ts.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c0b97a-2b29-43f2-80c4-32d85dcbbb61",
   "metadata": {},
   "source": [
    "Let's compare the accuracy of Linear Regression and Neural Network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b306a3-f766-4d71-af24-075eb6f8f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation train\", np.corrcoef(res_train, res_train_ANN)[0,1])\n",
    "print(\"Correlation train\", np.corrcoef(res_test, res_test_ANN)[0,1])\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, res_test_ANN))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, res_test_ANN))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, res_test_ANN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f48985e-f315-4102-83f5-8a4c9f62a00f",
   "metadata": {},
   "source": [
    "You can see, that the results we got for Neural Network are a little better than ones for Linear Regression. Let's try to use recurrent neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1583621d-e7f8-4d0f-8392-b0bde215289c",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory - LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4ead6-31fe-4213-b909-454969ed7d83",
   "metadata": {},
   "source": [
    "Unlike standard feedforward neural networks, an [**LSTM**](https://en.wikipedia.org/wiki/Long_short-term_memory) has feedback connections. It can not only process single data points, but also entire sequences of data (such as speech, video or time series). \n",
    "\n",
    "In the case of a time series, the neural network has one input and one output. However, the vector of time series values for the previous moments of time is fed to the input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e17844f-002f-4654-bb27-a9e19910463e",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0BOFEN/RNN.png\" width=\"1000\" alt=\"cognitiveclass.ai logo\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7bf9c-c8dc-4427-9790-ae3bd64fa4ad",
   "metadata": {},
   "source": [
    "To do this, we should transform the input DataSets into 3D shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8063fdc-d9b0-44b7-9e11-cac62687119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_LSTM = x_train.reshape((x_train.shape[0], 1, 4))\n",
    "test_x_LSTM = x_test.reshape((x_test.shape[0], 1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d8ff57-b9f1-414b-8b40-e02079cdbd5f",
   "metadata": {},
   "source": [
    "Let's create an LSTM Neural Network that consists of one [**LSTM**](https://keras.io/api/layers/recurrent_layers/lstm/) layer and one BP layer like in the previous case.\n",
    "As you can see, in this case our NN will consist of 100 LSTM and 100 BP neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f109a11b-0488-4dd0-abfc-e97e8258b386",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=int(y_train.shape[0]*.1)\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(train_x_LSTM.shape[1], train_x_LSTM.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_train.shape[1])) #activation='sigmoid'\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404c12e6-2a79-40b2-bf13-2dbd8a72507f",
   "metadata": {},
   "source": [
    "All subsequent steps of learning and predicting are similar to the previous neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3313f1d-bf35-4b7c-9101-2da06ff79f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_x_LSTM, y_train, epochs=epochs, batch_size=batch_size, validation_data=(test_x_LSTM, y_test), verbose=1, shuffle=False, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3709cd33-1968-4353-b3c2-b4f8f7fc7595",
   "metadata": {},
   "source": [
    "Let's plot the dynamic of loss and val loss like in the previous case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97ec3b-7901-4982-a91e-71995073bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##YOUR CODE GOES HERE## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f2b97-69bf-489b-8eed-8a908b6d3eb9",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa33d9b7-5c1e-4fb4-8bd9-79b04c8881b6",
   "metadata": {},
   "source": [
    "Let's calculate our forecast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d79e190-e535-4a39-9f1c-a271b67c88ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "##YOUR CODE GOES HERE## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e0645-2c4a-45df-953b-600d1abc8308",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>\n",
    "res_tr_LSTM = model.predict(train_x_LSTM)\n",
    "res_ts_LSTM = model.predict(test_x_LSTM)\n",
    "res_train_LSTM=scaler_y.inverse_transform(res_tr_LSTM).flatten()\n",
    "res_test_LSTM=scaler_y.inverse_transform(res_ts_LSTM).flatten()\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2c4865-0bab-4f00-93c8-76b73fb59cdb",
   "metadata": {},
   "source": [
    "And accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d94a2-9569-4c39-8d56-653936ddf4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##YOUR CODE GOES HERE## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d0252-286b-4b6d-964b-6e3bf90440dd",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>\n",
    "print(\"Correlation train\", np.corrcoef(res_train, res_train_LSTM)[0,1])\n",
    "print(\"Correlation train\", np.corrcoef(res_test, res_test_LSTM)[0,1])\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, res_test_LSTM))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, res_test_LSTM))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, res_test_LSTM)))\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14121475-0457-452e-874f-dbe82ca5cb8a",
   "metadata": {},
   "source": [
    "As you can see, the forecast results of the test data set are the same like in the previous models. Let's visualize these 3 results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad4bbd-f017-4fa5-8f73-ce50182f362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pred_test_ln = pd.Series(y_pred_test_ln, name = 'Predicted test Linear Model')\n",
    "res_pred_test_ANN = pd.Series(res_test_ANN, name = 'Predicted test ANN')\n",
    "res_pred_test_LSTM = pd.Series(res_test_LSTM, name = 'Predicted test LSTM')\n",
    "\n",
    "df_2 = pd.DataFrame({'Actual test': res_test, 'Linear Model': res_pred_test_ln, 'ANN Model': res_pred_test_ANN,  'LSTM Model': res_pred_test_LSTM,})\n",
    "df_2.index = dataset.index[len(dataset)-len(res_test):]\n",
    "df_2.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d0467-cba2-4965-8698-2a6c4d6fa2c3",
   "metadata": {},
   "source": [
    "As you can see, all forecasting shows similar results. \n",
    "\n",
    "None of the models can predict large peaks. However, the positions of the peaks coincide for all the models. That is, this approach allows you to make adequate models. The accuracy of the forecast depends on additional factors which you will try to consider in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1fe0b8-1738-401b-b467-5d5eec1fe495",
   "metadata": {},
   "source": [
    "## Model the effects of markdowns on holiday weeks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58764ca-a26d-451b-9372-2a8e33aba23c",
   "metadata": {},
   "source": [
    "### Preliminary analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9fff6-cc30-4961-bc81-47587347cdcf",
   "metadata": {},
   "source": [
    "To take into account the impact of markdowns on sales on holidays, we should first build a model of sales forecasting depending on other input parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e7dbb-2f69-4fbe-83f7-57fcb64847d2",
   "metadata": {},
   "source": [
    "Let's set Date as the index field in our DataSet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6b097-d6eb-427c-aa73-063c7d8a67c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = df_d.set_index('Date')\n",
    "df_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d84f7-3964-45bb-9ebe-4b99de0be9e9",
   "metadata": {},
   "source": [
    "Next, we should leave only those fields that affect weekly sales and remove the others. In particular, fields such as 'Store', 'Dept', 'Type' are for information only. Field 'Size' remains a constant for a specific department, and therefore cannot be used for modeling, even if it affects the sales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b656dbf-928b-462e-92ab-b8c5e4c42ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a170d1-5821-476a-bde6-60d5440f1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = df_d[['Weekly_Sales', 'IsHoliday', 'Temperature',\n",
    "       'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4',\n",
    "       'MarkDown5', 'CPI', 'Unemployment']]\n",
    "df_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f763cc-11e6-4005-9270-72b35953e46a",
   "metadata": {},
   "source": [
    "Let's use the function my_headmap from Lab2 to investigate the correlation between these fields:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db042da3-a327-43af-aa7f-af276c52b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_headmap(corr):\n",
    "    '''\n",
    "    Input:\n",
    "    corr: correlation matrix in DataFrame\n",
    "    '''\n",
    "    # Generate a mask for the upper triangle because it contains duplicate information\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap='RdYlGn', vmin=-1., vmax=1., annot=True, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c197447c-508a-4830-bc62-f47f0f22f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_headmap(df_d.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fd8d89-952a-485d-b451-99372280e132",
   "metadata": {},
   "source": [
    "As you can see there are no fields that lineary impact on Weekly Sales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b78e05c-5827-4300-80dc-b6b67513f0bc",
   "metadata": {},
   "source": [
    "Let's create our DataSet. To do this, join our historical 4 weeks sales data to this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e54b76-e8a8-46c5-99af-7041f1fcc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hp = df_d.join(dataset[dataset.columns[1:-1]])\n",
    "df_hp = df_hp.dropna()\n",
    "df_hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e4a6da-af5a-400d-9af1-64e9167dd85c",
   "metadata": {},
   "source": [
    "Let's create the input and target fields:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22523b77-cf5b-46fe-aaff-015cc5cf67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = df_hp.columns\n",
    "X, Y = df_hp[col[1:]], df_hp[col[0]]\n",
    "print(\"Input: \", X.columns)\n",
    "print(\"Target:\", Y.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b774ae-26ca-40dd-b9bb-614f00951284",
   "metadata": {},
   "source": [
    "Normalize them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf5f98-80da-40e0-be8c-bc3007508681",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scaled_x = ##YOUR CODE GOES HERE## \n",
    "scaled_y = ##YOUR CODE GOES HERE## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9fd3cf-cb5f-4c10-8476-945e3eaf123a",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>\n",
    "scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "scaled_x = scaler_x.fit_transform(X)\n",
    "scaled_y = scaler_y.fit_transform(Y.values.reshape(-1, 1))\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc2dda-34c2-4f9c-a938-452e06b526c5",
   "metadata": {},
   "source": [
    "And split them into training and test sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa3697-c2e0-452c-825c-77627405628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(scaled_x, scaled_y, test_size=0.3, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d896f5-2c48-40c9-a52e-663f97d8b186",
   "metadata": {},
   "source": [
    "We make inverse transform to get the training and test sets in real scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ed01f-cff3-4765-a3ce-285d5610510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train = scaler_y.inverse_transform(y_train).flatten()\n",
    "res_test = ##YOUR CODE GOES HERE## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f8742-a1e7-48f1-bc81-7af47f71fd7e",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>\n",
    "res_train = scaler_y.inverse_transform(y_train).flatten()\n",
    "res_test = scaler_y.inverse_transform(y_test).flatten()\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078edd9d-d4ac-45d4-a6c2-60fd946559ce",
   "metadata": {},
   "source": [
    "### Linear model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627f2e8-a426-416f-898c-be385f7b4e30",
   "metadata": {},
   "source": [
    "Let's create a Linear model for comparing the results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daaec7a-9e84-4bd5-a6cd-d3c81eb33872",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b41a5-d852-4065-a6e0-8e3909671707",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5cc16f-117d-40cd-912d-3e4c9b89601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_ln = regressor.predict(x_test)\n",
    "y_pred_test_ln = scaler_y.inverse_transform(y_pred_test_ln).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5080ab55-25dd-435e-9671-7c89a91d3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation train\", regressor.score(x_train, y_train))\n",
    "print(\"Correlation test\", regressor.score(x_test, y_test))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_test_ln))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_test_ln))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_ln)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2b807-0cf0-46c1-a39c-9005cd1b3e98",
   "metadata": {},
   "source": [
    "As you can see, the results are very bad too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1739e8c0-80f9-4171-b897-bb939e61f5ef",
   "metadata": {},
   "source": [
    "### Back propagation Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd57cab-142f-4ff0-ae39-5be24fc92a4a",
   "metadata": {},
   "source": [
    "Let's use the same Neural Network as in the previous task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd1da3-1a50-4c09-9ca1-c8270d4f7aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BP_model(X):\n",
    "    \"\"\"\n",
    "    Multilayer neural network with back propagation .\n",
    "    :param X: Input DataSet\n",
    "    :return: keras NN model\n",
    "    \"\"\"\n",
    "    # create model\n",
    "    model = Sequential() \n",
    "    model.add(Dense(100, input_dim=X.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbd1b2-8c23-4519-b00e-0f211bbeb00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size=int(y_train.shape[0]*.1)\n",
    "estimator = KerasRegressor(build_fn=BP_model, X=x_train, epochs=epochs, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9c10e-07cf-4e65-a797-710d9e17c3d7",
   "metadata": {},
   "source": [
    "We will use the same EarlyStopping function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c67dcb-60a4-49a9-b0df-594828641354",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='auto', patience=10, verbose=1, restore_best_weights=True)\n",
    "history=estimator.fit(x_train,y_train, validation_data=(x_test,y_test), callbacks=[es]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2859c8db-976c-4d66-97f0-7e299daf52e7",
   "metadata": {},
   "source": [
    "Let's show [**loss and validation loss dynamics**](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d981398-a79d-4fd0-b617-0333d215f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "##YOUR CODE GOES HERE## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7423ec3e-277d-4389-bcb8-82d8cfc1e341",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c40f6-831f-436c-ba55-b6fd4a57814c",
   "metadata": {},
   "source": [
    "As you can see, the Neural Network is fitting well and no overfitting is observed.\n",
    "Let's calculate the prediction of training (**res_train_ANN**) an test (**res_test_ANN**) sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ea7ac-d241-4913-b198-b1df495d0c3f",
   "metadata": {},
   "source": [
    "Let's calculate a forecast and make inverse normalization to real scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392878eb-ac37-4be3-bd93-4ac104413150",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tr=estimator.predict(x_train)\n",
    "res_ts=estimator.predict(x_test)\n",
    "res_train_ANN=scaler_y.inverse_transform(res_tr.reshape(-1, 1)).flatten()\n",
    "res_test_ANN=scaler_y.inverse_transform(res_ts.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebad4b8-a661-413e-bc9a-b4177a5c1965",
   "metadata": {},
   "source": [
    "Let's compare the accuracy of Linear Regression and Neural Network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5450f30-62a1-4791-8ba7-c008a317edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##YOUR CODE GOES HERE## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9781ea-8c66-487b-8525-a74ecd9a8510",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>\n",
    "print(\"Correlation train\", np.corrcoef(res_train, res_train_ANN)[0,1])\n",
    "print(\"Correlation train\", np.corrcoef(res_test, res_test_ANN)[0,1])\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, res_test_ANN))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, res_test_ANN))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, res_test_ANN)))\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855e855d-b1e8-4d2c-9816-565ff6aa05ea",
   "metadata": {},
   "source": [
    "As you can see, the forecast results of the test data set are much better than ones of the previous models. Let's visualize these 2 results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918f636-266e-40f3-bd8f-a7e6e65b9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pred_test_ln = pd.Series(y_pred_test_ln, name = 'Predicted test Linear Model')\n",
    "res_pred_test_ANN = pd.Series(res_test_ANN, name = 'Predicted test ANN')\n",
    "\n",
    "df_2 = pd.DataFrame({'Actual test': res_test, 'Linear Model': res_pred_test_ln, 'ANN Model': res_pred_test_ANN})\n",
    "df_2.index = df_d.index[len(df_d)-len(res_test):]\n",
    "df_2.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd31fc-090b-4509-b9c9-58072e84b046",
   "metadata": {},
   "source": [
    "As you can see from the plot, an ANN shows better results. \n",
    "\n",
    "Let's calculate the sensitivity of week sales for other factors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993589b-e911-4863-979b-7cbded4b6f9f",
   "metadata": {},
   "source": [
    "### Sensitivity analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbcdf5e-9035-432d-a0fe-4241c65092a3",
   "metadata": {},
   "source": [
    "We can modify the function from Lab2, adding regressor model as an input parameter. It will allow us to use this function for any types of regressors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62a322-8c20-4bbd-bf81-ac466e4d9f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sens(regressor, x, c, p):\n",
    "    '''\n",
    "    Input:\n",
    "    x: DataFrame of input Linear Regression\n",
    "    y: Series of output Linear Regression\n",
    "    p: Percentage of price change\n",
    "    Return:\n",
    "    Sensitivity of target\n",
    "    '''           \n",
    "    X = x[-1:].copy()\n",
    "    y_pred = regressor.predict(X)\n",
    "    X[0][c] = X[0][c]*(1+p)\n",
    "    y_pred_delta = regressor.predict(X)\n",
    "    return ((y_pred_delta - y_pred) / y_pred)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb0d2d0-9d2a-45d0-8402-7c2253adb753",
   "metadata": {},
   "source": [
    "Let's calculate the sensitivity of weekly sales for the last day in the DataSet with an alternate increase in the input parameters by 10%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e1b56-b294-417f-9b7e-a08bdc0a17ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,c in enumerate(df_hp.columns[2:]):\n",
    "    print(\"Sensitivity of Week Sales on %s: %5.2f%%\" % (c, my_sens(estimator, x_test, i+1,  0.1) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b20aaf9-9958-4bd0-a9ec-7060e3f9b87e",
   "metadata": {},
   "source": [
    "As can be seen from the results, this department is not sensitive to the impact of discounts on weekdays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a945d6-ea96-43e8-ac6a-8a4f752b7044",
   "metadata": {},
   "source": [
    "Let's analyze the impact of markdowns during the holiday week. To do this, we will create an input matrix that contains only information about the holidays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb780ddc-f354-4359-96f0-a643992e70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test2 = [list(x) for x in x_test if x[0]>=0.99]\n",
    "x_test2 = np.array(x_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3517abfa-a5d3-4764-80d8-5d832491dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,c in enumerate(df_hp.columns[2:]):\n",
    "    print(\"Sensitivity of Week Sales in Holiday on %s: %5.2f%%\" % (c, my_sens(estimator, x_test2, i+1,  0.1) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337914a5-3bec-4ef6-ae09-77ba704b6497",
   "metadata": {},
   "source": [
    "As you can see, the holiday week is not sensitive for markdowns too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b8d2e4-c008-4116-9f54-a4d59b09b174",
   "metadata": {},
   "source": [
    "## Recommendation for department\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8333a5c2-f78a-406c-9f64-3b281926c7fd",
   "metadata": {},
   "source": [
    "As can be seen from the sensitivity analysis for this department, the most significant is the MarkDown5. The other types of discounts either do not affect or, conversely, can have the opposite effect (MarkDown1). \n",
    "\n",
    "A very interesting is that the sales of this department are very sensitive to temperature. Along with the temperature increase, sales increase sharply both in the holiday and regular weeks. Therefore, the weather forecast should be taken into account in this case. \n",
    "\n",
    "It can also be seen that the sales intensity of this department have 2 weeks cycle, which is probably related to the type of goods. This means that sales increase will stimulate future sales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a437e634-af0b-48c2-b775-5b38160a37ce",
   "metadata": {},
   "source": [
    "## Final Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4837ad8b-b479-4a1d-bc1c-f1fc63eb7b14",
   "metadata": {},
   "source": [
    "In this lab, we investigated one department. Try to create a function and script that will allow you to perform a sensitivity analysis for any department of the specified store. This task can be divided into the following subtasks:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0554d5d-994b-4198-a770-2e2adc10567b",
   "metadata": {},
   "source": [
    "1. Create a function that will analyze the sensitivity of weekly sales in holiday days for any department.\n",
    "2. Apply this function for one department on your choice.\n",
    "3. Calculate the sensitivity for any 10 departments, that have 143 rows in the DataSet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb731ac6-ed0f-4bae-a1fe-37067ce0975c",
   "metadata": {},
   "source": [
    "### SubTask 1. Sensitivity function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e322abee-9878-4169-82a9-bfb8a7093a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sens_holiday(df, St, Dt):\n",
    "    # DataSet creation\n",
    "    ##YOUR CODE GOES HERE## \n",
    "    \n",
    "    # Week Sales Time Series creation\n",
    "    ##YOUR CODE GOES HERE## \n",
    "    \n",
    "    # Week Sales DataSet creation\n",
    "    ##YOUR CODE GOES HERE## \n",
    "    \n",
    "    # Splitting on Input and Target\n",
    "    ##YOUR CODE GOES HERE## \n",
    "    \n",
    "    # Normalization\n",
    "    ##YOUR CODE GOES HERE##     \n",
    "    \n",
    "    # Creation Train and Test DataSets\n",
    "    ##YOUR CODE GOES HERE## \n",
    "    \n",
    "    # Real scale target\n",
    "    ##YOUR CODE GOES HERE## \n",
    "    \n",
    "    # ANN Creation and fitting\n",
    "    ##YOUR CODE GOES HERE## \n",
    "    \n",
    "    # Creation Holidays DataSet\n",
    "    ##YOUR CODE GOES HERE## \n",
    "    \n",
    "    # Sensitivity calculation\n",
    "    ##YOUR CODE GOES HERE## \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3739bf0-3b70-43fd-b5bd-eb22a1530e26",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution DataSet creation</summary> \n",
    "<code>\n",
    "    df_d = df[(df['Store']==St) & (df['Dept']==Dt)]\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f5ad8-db87-490c-9c34-c6fbf61b0141",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution Week Sales Time Series creation</summary> \n",
    "<code>\n",
    "    ts = df_d[['Date', 'Weekly_Sales']]\n",
    "    ts = ts.set_index('Date')\n",
    "    ts = ts['Weekly_Sales']\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d353c6-84f1-448d-b82d-ba1676c91b89",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution Week Sales DataSet creation</summary> \n",
    "<code>\n",
    "    ts_dataset = series_to_supervised(pd.DataFrame(ts), ts, 4)\n",
    "    df_d = df_d.set_index('Date')\n",
    "    df_d = df_d[['Weekly_Sales', 'IsHoliday', 'Temperature',\n",
    "       'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4',\n",
    "       'MarkDown5', 'CPI', 'Unemployment']]\n",
    "    df_hp = df_d.join(ts_dataset[ts_dataset.columns[1:-1]])\n",
    "    df_hp = df_hp.dropna()\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ab1ad5-8225-4035-9834-c5dee8af0541",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution Splitting on Input and Target</summary> \n",
    "<code>\n",
    "    col = df_hp.columns\n",
    "    X, Y = df_hp[col[1:]], df_hp[col[0]]\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10973216-8982-4369-a685-f79ed0ac4ec7",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution Normalization</summary> \n",
    "<code>\n",
    "    scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_x = scaler_x.fit_transform(X)\n",
    "    scaled_y = scaler_y.fit_transform(Y.values.reshape(-1, 1))   \n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c59b8-2890-4eb2-9f26-a0a28c92be0f",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution Creation Train and Test DataSets</summary> \n",
    "<code>\n",
    "    x_train, x_test, y_train, y_test = train_test_split(scaled_x, scaled_y, test_size=0.3, shuffle=False)\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1c3b3-9cf9-499a-9dc2-c0f3edf8ab05",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution Real scale target</summary> \n",
    "<code>\n",
    "    res_train = scaler_y.inverse_transform(y_train).flatten()\n",
    "    res_test = scaler_y.inverse_transform(y_test).flatten()\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e6457d-ba1d-4497-8080-1526b820876a",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution ANN Creation and fitting</summary> \n",
    "<code>\n",
    "    epochs = 1000\n",
    "    batch_size=int(y_train.shape[0]*.1)\n",
    "    estimator = KerasRegressor(build_fn=BP_model, X=x_train, epochs=epochs, batch_size=batch_size, verbose=0) \n",
    "    es = EarlyStopping(monitor='val_loss', mode='auto', patience=10, verbose=1, restore_best_weights=True)\n",
    "    history=estimator.fit(x_train,y_train, validation_data=(x_test,y_test), callbacks=[es]) \n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba36f1d-cee6-460b-ada5-cc23a2293c92",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution Creation Holidays DataSet</summary> \n",
    "<code>\n",
    "    x_test2 = [list(x) for x in x_test if x[0]>=0.99]\n",
    "    x_test2 = np.array(x_test2)\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc738109-84a3-4d53-8d10-2edebe795a63",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution Sensitivity calculation</summary> \n",
    "<code>\n",
    "    res = {}\n",
    "    res['Store'] = [St]\n",
    "    res['Department'] = [Dt]\n",
    "    for i,c in enumerate(df_hp.columns[2:]):\n",
    "        res[c] = [\"{:.2f}%\".format(my_sens(estimator, x_test2, i+1,  0.1)*100)]  \n",
    "    res = pd.DataFrame(res)\n",
    "    res = res.set_index(['Store', 'Department'])\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f9df1-8870-4efe-bb4f-c0d8957e4c6d",
   "metadata": {},
   "source": [
    "### SubTask 2. Sensitivity of Department\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56289489-f9af-4433-a0f8-abebb9560ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_holiday(df, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653c49d2-e100-46f2-a087-8dc4c78ae096",
   "metadata": {},
   "source": [
    "### SubTask 3. Sensitivity of 10 departments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63361ae0-0f44-4fda-916b-4194bccce952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter departments with 143 rows\n",
    "##YOUR CODE GOES HERE## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6af332-1791-4e6d-a924-085303948a01",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>\n",
    "depts = df[['Store', 'Dept']].value_counts()\n",
    "depts = depts[depts == 143]\n",
    "depts.name = 'rows'\n",
    "depts\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17129fbb-e9ba-4ebd-bafe-97dea5f50c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle depts\n",
    "##YOUR CODE GOES HERE## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce190d77-ced4-4f6f-a5b0-4a05c91ccca6",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>\n",
    "depts = depts.reset_index()\n",
    "shuffled_dt = depts.reindex(np.random.permutation(depts.index))\n",
    "shuffled_dt\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958534c2-ef78-40e8-bd60-374c29a82697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity calculation \n",
    "##YOUR CODE GOES HERE## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f4c92-f09d-4f8c-a378-94cc5ddc8488",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>\n",
    "sens = pd.DataFrame()\n",
    "for v in shuffled_dt.values[:10]:\n",
    "    print('Store:', v[0], 'Department:', v[1])\n",
    "    sens = sens.append(sens_holiday(df, v[0], v[1]))    \n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb9418a-5412-4abc-bc1c-fa0b5813529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb531e9-2e4b-40b6-9d57-b07b79a72f45",
   "metadata": {},
   "source": [
    "## Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e408c-cd72-436b-b6be-1e608d4a83f3",
   "metadata": {},
   "source": [
    "During this lab, we learned how to analyze and forecast store sales.\n",
    "\n",
    "It was shown how to use autocorrelation analysis to find time lag delays. We studied how to transform a DataSet to take into account time delays in data.\n",
    "It was shown how to use linear models, backpropagation neural networks and recurrent neural networks to predict time series on the example of week sales of the store department.\n",
    "\n",
    "It was shown how to build combined DataSets containing both lag delays and store activity data.\n",
    "On the basis of a neural network, the influence of markdowns in the store on sales both during the holiday and regular weeks was analyzed.  A sales strategy for a specific department was proposed.\n",
    "\n",
    "At the end of the lab, the student should write a separate function that analyzes the impact of markdowns during the holiday weeks for a store department, which is transmitted as an input parameter. Based on this function, students should analyze the activities of 10 arbitrary departments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0868aeb-2d19-44f5-966e-1efd82f1676f",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ea8c5-2d02-4943-b38f-3194b49a7d41",
   "metadata": {},
   "source": [
    "Developer: [Yaroslav Vyklyuk, prof., PhD., DrSc](https://author.skills.network/instructors/yaroslav_vyklyuk_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff260ab7-5f16-4f3d-aa8b-f3bdd37b35de",
   "metadata": {},
   "source": [
    "Retail Consultant: [Olha Vdovichena, ass. prof, PhD](https://author.skills.network/instructors/olha_vdovichena)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeddc584-915e-45b1-9e6b-d53b381c47a3",
   "metadata": {},
   "source": [
    " Copyright &copy; 2020 IBM Corporation. This notebook and its source code are released under the terms of the [MIT License](https://cognitiveclass.ai/mit-license/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
